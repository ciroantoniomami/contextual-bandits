{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (LInUCB.py, line 19)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/jovoni/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-3a9168b8aae3>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from LinTS import *\n",
      "  File \u001b[1;32m\"/home/jovoni/Desktop/UNITS/Reinforcement_Learning/contextual-bandits/LinTS.py\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from main import get_data, compute_regret\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jovoni/Desktop/UNITS/Reinforcement_Learning/contextual-bandits/main.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from LInUCB import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/jovoni/Desktop/UNITS/Reinforcement_Learning/contextual-bandits/LInUCB.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    self.b = [np.zeros((self.d) for _ in range(self.K)]\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "from LogReg_TS import *\n",
    "from LinTS import *\n",
    "from Bernoulli_TS import *\n",
    "from LInUCB import *\n",
    "from tqdm import tqdm\n",
    "from utils import get_data, compute_regret\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"] = (16,9)\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_batch, user_feature, reward_list, action_context = get_data()\n",
    "action_context = np.array(action_context.iloc[:,2:])\n",
    "\n",
    "K, D = action_context.shape\n",
    "T = len(streaming_batch)\n",
    "T = 15000\n",
    "starting_point = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+starting_point+1, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+starting_point+1, 0])]\n",
    "    \n",
    "    optimal_action = np.random.randint(K)\n",
    "\n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1.0\n",
    "        regret = 0.0\n",
    "    else:\n",
    "        reward = 0.0\n",
    "        regret = 1.0\n",
    "\n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "            \n",
    "cum_reg_random = compute_regret(seq_error)\n",
    "seq_err_random = seq_error\n",
    "print(cum_reg_random[-1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "# Init bandit\n",
    "bandit = Bernoulli_TS(alpha=alpha, beta=beta, K=K)\n",
    "\n",
    "# \n",
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+1+starting_point, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+1+starting_point, 0])]\n",
    "    \n",
    "    optimal_action = bandit.get_action()\n",
    "    \n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1.0\n",
    "        regret = 0.0\n",
    "    else:\n",
    "        reward = 0.0\n",
    "        regret = 1.0\n",
    "\n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "\n",
    "    bandit.update(reward, optimal_action)\n",
    "\n",
    "cum_reg_bernoulli = compute_regret(seq_error)\n",
    "seq_err_bernoulli = seq_error\n",
    "print(cum_reg_bernoulli[-1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 1\n",
    "lambda_ = 1\n",
    "\n",
    "# Init bandit\n",
    "bandit = LinUCB(alpha, D, T, K, lambda_, disjoint=False)\n",
    "\n",
    "# \n",
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+1+starting_point, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+1+starting_point, 0])]\n",
    "    context = feature_user * action_context\n",
    "    \n",
    "    optimal_action = bandit.get_action(context)\n",
    "\n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1.0\n",
    "        regret = 0.0\n",
    "    else:\n",
    "        reward = 0.0\n",
    "        regret = 1.0\n",
    "\n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "\n",
    "    bandit.update(reward, context, optimal_action)\n",
    "\n",
    "cum_reg_LinUCB = compute_regret(seq_error)\n",
    "seq_err_LinUCB = seq_error\n",
    "print(cum_reg_LinUCB[-1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 1\n",
    "lambda_ = 1\n",
    "\n",
    "# Init bandit\n",
    "bandit = LinUCB(alpha, D, T, K, lambda_, disjoint=True)\n",
    "\n",
    "# \n",
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+1+starting_point, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+1+starting_point, 0])]\n",
    "    context = np.repeat(feature_user, K).reshape(K, D)\n",
    "    \n",
    "    optimal_action = bandit.get_action(context)\n",
    "\n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1.0\n",
    "        regret = 0.0\n",
    "    else:\n",
    "        reward = 0.0\n",
    "        regret = 1.0\n",
    "\n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "\n",
    "    bandit.update(reward, context, optimal_action)\n",
    "\n",
    "cum_reg_LinUCB_disjoint = compute_regret(seq_error)\n",
    "seq_err_LinUCB_disjoint = seq_error\n",
    "print(cum_reg_LinUCB_disjoint[-1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "R = 0.1\n",
    "delta = 0.9\n",
    "\n",
    "# Init bandit\n",
    "bandit = LinTS(R, delta, D, K)\n",
    "\n",
    "# \n",
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+1+starting_point, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+1+starting_point, 0])]\n",
    "    #context = feature_user * action_context\n",
    "    context = np.repeat(feature_user, K).reshape(K, D)\n",
    "    \n",
    "    optimal_action = bandit.get_action(mtx_content=context, t_step=t+1)\n",
    "\n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1\n",
    "        regret = 0\n",
    "    else:\n",
    "        reward = 0\n",
    "        regret = 1\n",
    "\n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "\n",
    "    bandit.update(reward=reward, mtx_content=context, optimal_action=optimal_action)\n",
    "    \n",
    "cum_reg_linear_ts = compute_regret(seq_error)\n",
    "seq_err_linear_ts = seq_error\n",
    "print(cum_reg_linear_ts[-1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lamba_ = 0.9\n",
    "alpha = 0.9\n",
    "S = 1000\n",
    "X_in = None\n",
    "y_out = None\n",
    "\n",
    "# Init bandit\n",
    "bandit = LogReg_TS(lambda_=lambda_, D=D, alpha=alpha)\n",
    "\n",
    "# \n",
    "seq_error = np.full(T-1-starting_point,0)\n",
    "for t in tqdm(range(0, T-1-starting_point)):\n",
    "    feature_user = np.array(user_feature[user_feature.index == int(streaming_batch.iloc[t+1+starting_point, 0])])\n",
    "    watched_list = reward_list[reward_list['user_id'] == int(streaming_batch.iloc[t+1+starting_point, 0])]\n",
    "    context = feature_user * action_context\n",
    "    #context = np.repeat(feature_user, K).reshape(K, D)\n",
    "    \n",
    "    optimal_action = bandit.get_action(context)\n",
    "    \n",
    "    if optimal_action in list(watched_list['movie_id']):\n",
    "        reward = 1\n",
    "        regret = 0\n",
    "    else:\n",
    "        reward = 0\n",
    "        regret = 1\n",
    "        \n",
    "    if X_in is not None:\n",
    "        X_in = np.vstack((X_in, context[optimal_action]))\n",
    "        y_out = np.append(y_out, reward)\n",
    "    else:\n",
    "        X_in = context[optimal_action]\n",
    "        y_out = np.array([reward])\n",
    "        \n",
    "    if t == 0:\n",
    "            seq_error[t] = regret\n",
    "    else:\n",
    "            seq_error[t] = seq_error[t-1] + regret\n",
    "\n",
    "    if t % S == 0:\n",
    "        bandit.update(X_in, y_out)\n",
    "    \n",
    "cum_reg_logreg_ts = compute_regret(seq_error)\n",
    "seq_err_logreg_ts = seq_error\n",
    "print(cum_reg_logreg_ts[-1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(T-1-starting_point)]\n",
    "plt.plot(x, cum_reg_random, label=\"Random\")\n",
    "plt.plot(x, cum_reg_bernoulli, label=\"Bernoulli\")\n",
    "plt.plot(x, cum_reg_linear_ts, label=\"Linear TS\")\n",
    "plt.plot(x, cum_reg_logreg_ts, label=\"LogReg TS\")\n",
    "plt.plot(x, cum_reg_LinUCB, label=\"Linear UCB\")\n",
    "plt.plot(x, cum_reg_LinUCB_disjoint, label=\"Linear UCB disj\")\n",
    "plt.legend()\n",
    "#plt.ylim(-0.05,1.05)\n",
    "#plt.savefig(f\"plots/T_{T}_onlyuser={only_user_context}.svg\", facecolor=\"white\", bordercolor=\"white\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a83d29bc878340e2020b93292cd2f1fc60b1b21ca7c9c06a98e623e6dafee45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
